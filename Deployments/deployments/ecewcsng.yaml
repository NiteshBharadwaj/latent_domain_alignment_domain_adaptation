apiVersion: v1
kind: Pod
metadata:
  name: debug-gpu # replace with your preferred log-in node name
# Request at least 4 CPU and 20Gi mem per GPU.
# To be of the safe side, request 24Gi mem per GPU in limit.
spec:
  containers:
  - name: login
    image: gitlab-registry.nautilus.optiputer.net/yol070/research-containers/research-pytorch1.0-cuda10
    imagePullPolicy: Always
    args: ["sleep", "infinity"]
    resources:
      requests:
        memory: "20"
        cpu: "16"
      limits:
        nvidia.com/gpu: 1 # requesting X GPU
        memory: "20"
        cpu: "16"
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
    - mountPath: /ceph
      name: ceph
  restartPolicy: Never
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
    - name: ceph
      flexVolume:
        driver: ceph.rook.io/rook
        fsType: ceph
        options:
          clusterNamespace: rook
          fsName: nautilusfs
          path: /ecewcsng
          mountUser: ecewcsng
          mountSecret: ceph-fs-secret
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu-type
            operator: In # Use NotIn for other types
            values:
            - "1080Ti" 
