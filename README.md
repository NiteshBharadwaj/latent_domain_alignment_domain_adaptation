# Moment Matching for Multi-Source Domain Adaptation
<img src='https://github.com/VisionLearningGroup/VisionLearningGroup.github.io/blob/master/M3SDA/imgs/overview.png'>

Ran on:
Python 3.6, Torch 1.0. Pretty sure it will run on > 1.0, and they recommend >0.3

GPU: 8Gi, CPU: 8Gi, Batch Size: 128, 
    Training Time USPS: < 5min Acc 96% (Same as paper in 3 epochs) 
    Training Time MNIST-M: < 2hrs Acc 73% (Same as in paper in 80 epochs)
     

Pip cmds:
    
    pip install git+https://github.com/pytorch/tnt.git@master

CompCar Setup:
i) In 'data' folder create 'CCWeb' and 'CCSurv' sub folders
ii) From main directory, run 
```
bash download_cc.sh
cd data/CCSurv/
cat sv_data.z* > sv_data_combined.zip
unzip -P  d89551fd190e38 sv_data_combined.zip
cd ..
cd data/CCWeb/
cat data.z* > data_combined.zip
unzip -P  d89551fd190e38 data_combined.zip
```




To reproduce Digit-5 Results:

i) Download dataset from here: https://drive.google.com/open?id=1A4RJOFj4BJkmliiEL7g9WzNIDUHLxfmm

If you're running headless, use the following snippet: 

    ggID='1A4RJOFj4BJkmliiEL7g9WzNIDUHLxfmm'  
    ggURL='https://drive.google.com/uc?export=download'  
    filename="$(curl -sc /tmp/gcokie "${ggURL}&id=${ggID}" | grep -o '="uc-name.*</span>' | sed 's/.*">//;s/<.a> .*//')"  
    getcode="$(awk '/_warning_/ {print $NF}' /tmp/gcokie)"  
    curl -Lb /tmp/gcokie "${ggURL}&confirm=${getcode}&id=${ggID}" -o "${filename}"  
    
ii) Create a folder "data" and "record" in main working directory, and extract contents of the downloaded zip file to data
iii) Run the following bash script

    bash experiment_do.sh  mnistm 100 0 record/mnistm_MSDA_beta

Change second argument to usps/svhn/syn to reproduce the corresponding results



Their README follows for reference:

PyTorch implementation for **Moment Matching for Multi-Source Domain Adaptation** (**ICCV2019 Oral**). This repository contains some code from [Maximum Classifier Discrepancy for Domain Adaptation](https://github.com/mil-tokyo/MCD_DA). If you find this repository useful for you, please also consider cite the MCD paper!


The code has been tested on Python 3.6+PyTorch 0.3. To run the training and testing code, use the following script:

## Installation
- Install PyTorch (Works on Version 0.3) and dependencies from http://pytorch.org.
- Install Torch vision from the source.
- Install torchnet as follows
```
pip install git+https://github.com/pytorch/tnt.git@master
```
## Digit-Five Download
Since many researchers have sent us emails for Digit-Five data. We share the Digit-Five dataset we use in our experiments in the following download link:

https://drive.google.com/open?id=1A4RJOFj4BJkmliiEL7g9WzNIDUHLxfmm

Keep in mind that the Mnist-M dataset is generated by ourselves, thus this subset may be different from the one released by DANN paper.

If you find the Digit-Five dataset useful for your research, please cite our paper.
## DomainNet
The DomainNet dataset can be downloaded from the following link:
[http://ai.bu.edu/M3SDA/](http://ai.bu.edu/M3SDA/)

We are also organizing a TaskCV and VisDA chanllenge in conjunction with ICCV 2019, Seoul, Korea, based on this dataset. See more details with the following link:
[http://ai.bu.edu/visda-2019/](http://ai.bu.edu/visda-2019/)

## Citation

If you use this code for your research, please cite our [paper](https://arxiv.org/pdf/1812.01754.pdf)
```
@article{peng2018moment,
        title={Moment Matching for Multi-Source Domain Adaptation},
        author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
        journal={arXiv preprint arXiv:1812.01754},
        year={2018}
        }
```
             
